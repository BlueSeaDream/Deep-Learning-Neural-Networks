深度学习面试100题

1.	梯度下降算法的正确步骤是什么？
a.	计算预测值和真实值之间的误差
b.	重复迭代，直至得到网络权重的最佳值
c.	把输入传入网络，得到输出值
d.	用随机值初始化权重和偏差
e.	对每一个产生误差的神经元，调整相应的（权重）值以减小误差
A.	abcde 	B. edcba 		C. cbaed 		D. dcaeb

解析：正确答案D

2.	训练CNN时，可以对输入进行旋转、平移、缩放等预处理提高模型泛化能力。这么说是对，还是不对？

A.对		B.不对

解析：对。训练CNN时，可以进行这些操作。当然也不一定是必须的，只是data augmentation扩充数据后，模型有更多数据训练，泛化能力可能会变强。

3.	下面哪项操作能实现跟神经网络中Dropout的类似效果？
a.	Boosting 
b.	Bagging 
c.	Stacking 
d.	Mapping

解析：正确答案b。Dropout可以认为是一种极端的Bagging，每一个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。

4.	下列哪一项在神经网络中引入了非线性？
a.	随机梯度下降
b.	修正线性单元（ReLU）
c.	卷积函数
d.	以上都不正确

解析：正确答案b,修正线性单元是非线性的激活函数。


问答题

1.	CNN的卷积核是单层的还是多层的？
解析：
深度卷积网络是由多层特征图构成, 存贮输入数据或其中间表示值。一组卷积核是联系前后两层的网络参数表达体。描述网络模型中某层的厚度，通常用channel数，数据输入的前层厚度M称之为通道数（比如RGB输入通道数为3），卷积输出的后层厚度称之为特征图数。卷积核一般是多层的，除了二维空间参数（如：3x3，5x5）之外, 还有厚度参数H。

卷积核的厚度H, 一般等于前层厚度M，特殊情况M > H。卷积核的个数N, 一般等于后层厚度。卷积核通常从属于后层，为后层提供各种前层特征的解释。卷积核厚度等于1时为2D卷积，是点积运算—对应平面点相乘然后把结果加起来；卷积核厚度大于1时为3D卷积，每片分别平面点求卷积，然后把每片结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例，有厚度无面积，直接把每片单个点乘以权重再相加。

2.	什么是卷积？
解析：
用固定的神经元权重的矩阵（滤波器）对图像进行滑动窗口的线性内积操作（（逐个元素相乘再求和））。

3.	什么是CNN的池化Pool层？
解析：
池化，即取滑动窗口区域的平均或最大值，用于缩小特征图和提取滑动窗口特征。

4.	简述下什么是生成对抗网络。
解析：
GAN的内部是竞争关系，一方叫Generator，尽量生成看上去是来自于训练样本的图片。另一方是Discriminator，判断输入图片是否属于真实训练样本。

5.	请简要介绍下tensorflow的计算图。
解析：
Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor（张量），而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)。

6.	在CNN和RNN网络训练中，你有哪些网络训练的调参经验？
解析：
a.	参数初始化：不可缺少的步骤，否则可能会减慢网络收敛速度，影响收敛结果，甚至造成Nan等一系列问题。采用下面几种参数初始化的方式，结果类似。
 (1)	Xavier：适用于普通激活函数（tanh，sigmoid：scale = np.sqrt(3/n) ）。
 (2)	He：适用于ReLU：scale = np.sqrt(6/n)。
 (3)	Uniform均匀分布初始化。
 (4)	Normal高斯分布初始化。
 (5)	SVD初始化：对RNN有比较好的效果。

b.	数据预处理方式
 (1)	Zero-Center：常用。X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalize
 (2)	PCA whitening：不常用。

c.	训练技巧：
 (1)	要做梯度归一化，即算出来的梯度除以minibatch size clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。
 (2)	Dropout：在小数据集训练时，防止过拟合，Dropout一般设为0.5，效果提升都明显。
 (3)	优化方法：在小数据训练数据上，Adam，Adadelta等优化方法不如Sgd, Sgd收敛速度会慢一些，但是最终收敛后的结果。使用Sgd优化方法时，选择学习率从1.0或者0.1开始，训练一段时间，在验证集上检查，如果cost没有下降，就对学习率减半。也可以先用Ada系列的优化方法先训练，快收敛时候，更换成Sgd继续训练，同样也会有提升。Adadelta一般在分类问题上效果比较好，Adam在生成问题上效果比较好。
 (4)	激活函数：采用tanh或者relu之类的激活函数，尽量不要用Sigmoid。因为， Sigmoid函数在-4到4的区间里，有较大的梯度，其它区域，梯度接近0，易造成梯度消失的问题。另外，输入为0均值，sigmoid函数的输出不是0均值。
 (5)	批处理：Batch Size一般从128左右开始调整，合适最重要，并不是越大越好。word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果。




7.	为什么引入非线性激励函数？
解析：
使用线性激活函数下，多层网络就是反复用矩阵去乘以输入，与一层网络相当,如多层感知机Perceptron。非线性相当于对空间进行变换，变换完成后使得原来线性不可解的问题，现在变得可以解了。下图可以形象地说明非线性变换，左图用一根线是无法划分的。经过一系列变换后，就变成线性可解的问题了。
 
引入非线性函数作为激励函数，深层神经网络不再是输入的线性组合，而是可以逼近任意函数。早期的激励函数是sigmoid函数或者tanh函数，输出有界，可以充当下一层输入。

8.	请问人工神经网络中为什么激励函数ReLu要好过于tanh和sigmoid函数？
解析：
 (1)	采用sigmoid和tanh函数，正向和反向传播时，涉及除法和指数运算，计算量大，而采用Relu激活函数，整个过程的计算量节省很多。
 (2)	在sigmoid接近饱和区时，函数值变换缓慢，导数趋于零，会造成信息丢失。对于深层网络，sigmoid函数在反向传播时，很容易出现梯度消失的情况，无法完成深层网络的训练这种现象称为饱和。而ReLU就不会有饱和倾向，不会有极小梯度的出现。
 (3)	Relu会使一部分神经元的输出为零，构成网络的稀疏，减少参数的相互依赖，从而缓解网络过拟合的发生。对relu的改进，如Prelu和Random Relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进。
 
除选用合适的激励函数，现在主流的做法会进行Batch Normalization，尽可能地保证每一层网络的输入具有相同的分布。

9.	为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择一种sigmoid或者tanh？这样做的目的是什么？
 

解析：
sigmoid用于各种gate上，产生0~1之间的值。 tanh 用于状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。二者目的不一样

10.	如何解决RNN梯度爆炸和弥散的问题？
解析：
为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述：
算法：当梯度爆炸时截断梯度，
 
下图为梯度截断的效果，它展示了一个RNN（其中W为权值矩阵，b为bias项）的决策面。这个模型是短时间RNN单元的组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型目标函数有较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。

 

为了解决梯度弥散的问题，我们介绍了两种方法。第一种方法是将随机初始化，改为一个有关联的矩阵初始化。第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，梯度传播时不会在一定时间之后变小。

11.	什么样的资料集不适合用深度学习？
解析：
 (1)	数据集太小：数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
 (2)	数据集没有局部相关特性：目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。例如，预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素。

12.	广义线性模型是怎被应用在深度学习中？
解析：
深度学习从统计学角度，可以看做递归的广义线性模型。广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。
深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。

下图是一个对照表：

 

13.	如何解决梯度消失和梯度膨胀？
解析：
梯度消失：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0
可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。关于深度学习中 Batch Normalization为什么效果好？
梯度膨胀:根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大,可以采用ReLU激活函数来解决，或用Batch Normalization解决这个问题。

14.	简述神经网络的发展历史。
解析：
1949年，Hebb提出了神经心理学学习范式—Hebbian学习理论。1952年，IBM的Arthur Samuel写出了西洋棋程序。1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型。3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好地应用到了感知器的训练中。感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。随着计算资源的增长和数据量的增长，一个新的NN领域——深度学习出现了。简言之，MP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+sigmoid—- (低谷) —>深度学习+pre-training+ReLU/sigmoid

15.	深度学习常用方法。
解析：
全连接DNN（相邻层相互连接、层内无连接）：AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—>特征探测器—>栈式叠加、贪心训练
RBM—->DBN
•	解决全连接DNN的全连接问题–>CNN
•	解决全连接DNN的无法对时间序列上变化进行建模的问题–>RNN—解决时间轴上的梯度消失问题->LSTM

DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。

CNN主要用于计算机视觉(Computer Vision)领域，CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。

GAN是一种应用在生成模型的训练方法，现在有很多在CV方面的应用，例如图像翻译，图像超清化、图像修复等等。

RNN主要用于自然语言处理(Natural Language Processing)领域，用于处理序列到序列的问题。普通RNN会遇到梯度爆炸和梯度消失的问题。所以现在在NLP领域，一般会使用LSTM模型。在最近的机器翻译领域，Attention作为一种新的手段，也被引入进来。

除了DNN、RNN和CNN外，自动编码器(AutoEncoder)、稀疏编码(Sparse Coding)、深度信念网络(DBM)、限制玻尔兹曼机(RBM)也都有相应的研究。

16.	神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？
解析：
•	非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。
•	几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响。
•	计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之类比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。
•	非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。
•	单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。
•	输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。
•	接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。
•	参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。
•	归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。

17.	梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？
解析：
深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，我们可能从来没有找到过“局部最优”，更别说全局最优了。很多人都有一种看法，就是“局部最优是神经网络优化的主要难点”。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型。在后两种情况下，是很难找到局部极值的，更别说全局最优了。
现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning（https://arxiv.org/abs/1611.07476）里面的实验研究给出以下的结论：
•	Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.
•	There are still negative eigenvalues even when they are small in magnitude.
另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。（https://arxiv.org/abs/1706.10239）
For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima.
所以，很可能我们实际上是在“什么也没找到”的情况下就停止了训练，然后拿到测试集上试试，“咦，效果还不错”。补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape 的鞍点数目指数增加，而具有较差loss的局部极值非常少。

18.	为什么很多做人脸的Paper会最后加入一个Local Connected Conv？
解析：
以FaceBook DeepFace 为例，DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

19.	什么是梯度爆炸？
解析：
误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

30题完成

